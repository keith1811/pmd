[tox]
envlist = py39
skipsdist = True

[flake8]
count = False
ignore = W293, W504
max-line-length = 120
statistics = True
exclude =
    .pipeline
    .tox
    conf
    config
    init_scripts
    integration_tests
    job_definitions
    ; modules
    ; myapp
    notebooks
    sqls
    ; src
    sample_data
    stages
    templates
    terraform
    ; tests
    ; unit_tests
    ; unit_tests.bak
    ; utils
    xunit
    .coverage
    .gitignore
    azure-pipelines.yml
    CODEOWNERS
    pytest.ini
    README.md
    requirements-ml.txt
    requirements.txt
    setup.py
    sonar-project.properties
    tox.ini
    VERSION
    version.txt
    wss-unified-agent.config

[testenv]
setenv =
    SPARK_HOME = {env:SPARK_HOME}
    PYSPARK_PYTHON = python
    PATH = {env:PATH}:{env:SPARK_HOME}/python:{env:SPARK_HOME}/bin:{env:SPARK_HOME}/sbin
    PYTHONPATH = {toxinidir}/modules:{toxinidir}/utils:{toxinidir}/unit_tests
    ; PIP_INDEX_URL = https://pypi.tuna.tsinghua.edu.cn/simple/
    PIP_EXTRA_INDEX_URL = https://{env:PIPER_VAULTCREDENTIAL_ARTIFACTORY_USER}:{env:PIPER_VAULTCREDENTIAL_ARTIFACTORY_PASSWORD}@common.repositories.cloud.sap/artifactory/api/pypi/build.releases.pypi/simple
deps =
    -r./requirements.txt
    -r./requirements-ml.txt
    -r./unit_tests/local_requirements.txt
    platform-datalake-jobs
commands =
    bandit -r notebooks
    bandit -r modules
    ; bandit -r utils
    ; flake8 notebooks modules utils
    pip install pyspark==3.3.2 delta-spark==2.3.0 --force-reinstall
    pytest --cov modules --cov utils --ignore=tests --ignore=integration_tests --ignore=unit_tests.bak --junitxml=xunit/TEST-results.xml --cov-report xml:xunit/coverage-reports/jacoco.xml